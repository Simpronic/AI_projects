# -*- coding: utf-8 -*-
"""ANN_build.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1msP20Mhj4G0tZtLm4pH7rZDxAkNkJ-yJ

Costruire una ANN per classificazione binaria con TF
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.preprocessing import LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, accuracy_score

dataset = pd.read_csv("Churn_Modelling.csv")
X = dataset.iloc[:, 3:-1].values
y = dataset.iloc[:, -1].values

le = LabelEncoder()
X[:, 2] = le.fit_transform(X[:, 2])

X[:, 1] = le.fit_transform(X[:, 1])

"""Per le nazioni utilizziamo un One-Hot encoder poichè un encoder classico come LabelEncoder potrebbe introdurre un bias nella rete poichè assegnerebbe numeri crescenti alle label, un one-hot encoder invece non provoca questo effetto

"""

ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [1])], remainder='passthrough')
X = np.array(ct.fit_transform(X))

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

print(X_train)

"""Effettuo uno scaling per evitare che le colonne con valore più alto possano sormontare le colonne con valore più basso, portando le features ad avere la stessa valenza iniziale"""

sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

print(X_train)

"""Per costruire la rete prendiamo le funzioni di attivazione relu (rectifier) poichè sono quelle più usate per costruire reti artificiali per i livelli nascosti.
Il numero di neuroni invece può essere considerato un iperparameto da ottimizzare, in questo caso euristicamente è stato scelto 6
"""

ann = tf.keras.models.Sequential()
ann.add(tf.keras.layers.Dense(units=6, activation='relu'))
ann.add(tf.keras.layers.Dense(units=6, activation='relu'))
ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))

"""Utilizzo la binary_crossentropy poichè è un problema di classificazione binaria, se avessi più classi posso usare la softmax"""

ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])

history = ann.fit(X_train, y_train, batch_size = 32, epochs = 100,validation_data=(X_test, y_test))

train_accuracy = history.history['accuracy']
val_accuracy = history.history['val_accuracy']
train_loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(train_accuracy) + 1)

plt.figure(figsize=(10, 6))
plt.plot(epochs, train_accuracy, label='Training Accuracy')
plt.plot(epochs, val_accuracy, label='Validation Accuracy')
plt.xlabel('Epoche')
plt.ylabel('Accuracy')
plt.title('Accuracy in funzione delle epoche')
plt.legend()
plt.grid(True)
plt.show()

plt.figure(figsize=(10, 6))
plt.plot(epochs, train_loss, label='Training Loss')
plt.plot(epochs, val_loss, label='Validation Loss')
plt.xlabel('Epoche')
plt.ylabel('Loss')
plt.title('Loss in funzione delle epoche')
plt.legend()
plt.grid(True)
plt.show()

"""Possiamo anche generare la matrice di confusione"""

y_pred = ann.predict(X_test)
y_pred = (y_pred > 0.5)

cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)